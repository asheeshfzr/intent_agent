---

# ‚ö° Intent-Routed Agentic AI System

### *Next-Gen LangGraph + LLM Orchestration Framework for Autonomous Multi-Tool Reasoning*

> ‚ÄúNot just an agent ‚Äî a thinking system that understands, plans, reasons, and adapts.‚Äù

---

## üß† Overview

This project is a **next-generation Agentic AI system** built with **LangGraph**, **LangChain**, and **Local LLMs**, designed to showcase how an **autonomous reasoning agent** can understand human intent, dynamically route tasks, invoke multiple specialized tools, and iteratively improve its reasoning through feedback loops.

It demonstrates the fusion of **Generative AI (LLMs)** and **Agentic AI frameworks** to create a system that can:

* Interpret a **natural language query**
* Choose the optimal **workflow and tools** via **LangGraph orchestration**
* Perform **multi-hop reasoning and inference**
* Adapt and re-route via **feedback and confidence-based recovery**
* Return **traceable, explainable outputs**

This is not just a POC ‚Äî it‚Äôs a **foundational blueprint** for building **autonomous AI agents** capable of **tool orchestration**, **self-healing reasoning**, and **contextual decision-making**.

---

## üß© Core Features

### üß≠ **LangGraph-Powered Orchestration**

The system leverages **LangGraph**, the latest evolution in LLM orchestration frameworks, to model workflows as **directed stateful graphs** ‚Äî giving fine-grained control over reasoning steps, node transitions, and failure recovery.

Each user query triggers a **dynamic agent graph**, composed of reasoning nodes:

```
understand ‚Üí route ‚Üí fetch ‚Üí infer ‚Üí finalize
```

These nodes can re-activate or redirect themselves based on intermediate results ‚Äî introducing **true agentic adaptability**.

---

### üß† **LLM-Driven Intent Router**

The **router** (using prompt from `router_v1.txt`) classifies the query into:

* `metrics_lookup`
* `knowledge_lookup`
* `calc_compare`
* `unknown`

It provides structured JSON with `intent`, `confidence`, and `entities` extracted (e.g., service name, duration window, etc.), enabling deterministic downstream planning.

---

### üß∞ **Multi-Tool Reasoning Framework**

Integrated via **LangChain + LangGraph hybrid nodes**, the agent dynamically selects tools to fulfill the task:

| Tool                      | Description                                                            | Technology                               |
| ------------------------- | ---------------------------------------------------------------------- | ---------------------------------------- |
| **Metrics Tool**          | Queries a REST/HTTP mock metrics API for service latency, errors, etc. | `httpx`, FastAPI                         |
| **Vector Knowledge Tool** | Performs vector-based doc search via **Qdrant** or TF-IDF fallback     | `qdrant-client`, `sentence-transformers` |
| **Utility SQL Tool**      | Runs analytical operations or comparison queries locally               | SQLite, LangChain Tool Interface         |

Each tool can be **composed** or **sequenced** by the LangGraph workflow for multi-step reasoning (e.g., comparison or aggregation tasks).

---

### üîÅ **Inference + Feedback Loops**

After execution, the system performs reasoning and validation:

* **Inference Engine:** Evaluates results (e.g., latency thresholds or reliability checks)
* **Feedback Loop:** Detects low-confidence responses, missing data, or ambiguous outputs
  ‚Üí triggers alternative tool paths or clarification requests.

This introduces **self-healing reasoning**, a core property of advanced Agentic AI systems.

---

### üßÆ **Local LLM Runtime**

* Uses **Llama.cpp (GGUF)** models for local inference (`llm_local.py`)
* Optimized for **fast, offline reasoning**
* Controlled generation (low temperature, short context)
* Seamlessly switches between **LangChain** and **LangGraph** execution modes depending on environment configuration

This design ensures privacy, cost efficiency, and consistent deterministic reasoning.

---

### üìä **Traceable AI Decisions**

Every node, reasoning step, and tool invocation is logged via `trace.py` and accessible via:

* `GET /trace` ‚Üí detailed provenance graph
* `POST /clear_trace` ‚Üí reset trace memory

The API also returns **compact inlined traces** for transparency and explainability.

---

## ‚öôÔ∏è Architecture Diagram

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ       User Query           ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  LLM Intent Router         ‚îÇ
                    ‚îÇ (router_v1.txt prompt)     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  LangGraph Orchestrator    ‚îÇ
                    ‚îÇ  (stateful agent graph)    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                       ‚îÇ                        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Metrics Tool    ‚îÇ     ‚îÇ Vector Search   ‚îÇ      ‚îÇ Utility SQL     ‚îÇ
‚îÇ (REST API)      ‚îÇ     ‚îÇ (Qdrant / TF-IDF)‚îÇ     ‚îÇ (Local calc)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                        ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñº                       ‚ñº
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚îÇ Inference + Feedback Loop          ‚îÇ
                 ‚îÇ (confidence checks, rerouting)     ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚îÇ Final Answer + Execution Trace      ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üß™ Example Queries

**1Ô∏è‚É£ Metrics Lookup**

```bash
curl -s -X POST "http://localhost:8000/query" \
-H "Content-Type: application/json" \
-d '{"query":"what is the p95 latency for service payments in last 5m?"}'
```

Response:

```json
{"answer":"payments p95=250ms > 200ms threshold","trace":["intent_router","metrics_fetch","inference"]}
```

**2Ô∏è‚É£ Knowledge Search**

```bash
curl -s -X POST "http://localhost:8000/query" \
-d '{"query":"how to configure SAML for internal dashboard?"}'
```

Response:

```json
{"answer":"Found doc: saml_setup.md - SAML configuration guide snippet...","trace":["intent_router","vector_search"]}
```

**3Ô∏è‚É£ Comparative Reasoning**

```bash
curl -s -X POST "http://localhost:8000/query" \
-d '{"query":"compare p95 latency between payments and orders"}'
```

Response:

```json
{"answer":"Payments=250ms, Orders=180ms, diff=70ms","trace":["intent_router","fetch_metrics","aggregate"]}
```

---

## üß∞ Tech Stack

| Layer                    | Technology                   | Purpose                         |
| ------------------------ | ---------------------------- | ------------------------------- |
| **Core Framework**       | Python 3.10+, FastAPI        | API & orchestration layer       |
| **LLM Runtime**          | llama-cpp-python             | Local inference engine          |
| **Agentic Graph Engine** | LangGraph 0.2.16             | Stateful agent orchestration    |
| **Reasoning Framework**  | LangChain                    | Tool abstractions + ReAct agent |
| **Vector Search**        | Qdrant / TF-IDF              | Knowledge retrieval             |
| **Data Models**          | Pydantic v2                  | Typed settings/configuration    |
| **Mock Services**        | FastAPI (docs & metrics)     | Simulation of external APIs     |
| **Observability**        | Trace logs + JSON provenance | Explainable agent behavior      |

---

## üß† Why This Matters

This project captures the **emergent capabilities of Agentic AI** ‚Äî moving from prompt-based LLMs to **goal-driven, tool-using, and self-correcting autonomous systems**.

By combining:

* **LLMs for reasoning**
* **LangGraph for planning**
* **LangChain for tool execution**
* **Qdrant for semantic memory**

‚Ä¶it builds the foundation of a **self-directed AI reasoning system** that can orchestrate data, tools, and logic ‚Äî just like a human operator.

---

## ‚ö° Getting Started

### 1. Setup environment

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. Configure

Edit `.env` and set:

```
GGML_MODEL_PATH=/path/to/llama-2-7b-chat.Q4_0.gguf
USE_QDRANT=true
USE_LANGCHAIN=true
```

### 3. Run locally

```bash
./start_local.sh
```

This script auto-starts:

* Metrics mock (port 9000)
* Docs mock (port 9010)
* Agent API (port 8000)

### 4. Optional: Run via Docker Compose

```bash
docker-compose up --build
```

---

## üìö API Endpoints

| Endpoint       | Method | Description                    |
| -------------- | ------ | ------------------------------ |
| `/query`       | POST   | Execute an agentic query       |
| `/trace`       | GET    | Retrieve full provenance graph |
| `/clear_trace` | POST   | Reset stored traces            |

---

## üß© Future Enhancements

* üîÆ Memory-augmented reasoning with persistent LangGraph states
* üïµÔ∏è Context-aware RAG pipelines
* üß† Agent collaboration graphs (multi-agent orchestration)
* ‚öôÔ∏è Advanced confidence-based recovery & self-critique reasoning

---

## üèÅ Summary

This project bridges **Generative AI** and **Agentic Intelligence**, turning LLMs from ‚Äúresponders‚Äù into **autonomous reasoning entities** capable of understanding goals, decomposing tasks, and acting with purpose.

It embodies the evolution from **prompt-based AI ‚Üí tool-using AI ‚Üí goal-oriented autonomous AI**.

---
